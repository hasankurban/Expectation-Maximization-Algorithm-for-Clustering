# i =2.5
range_of_matches[1,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<= i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[2,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[3,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<=i & data_to_split[,ncol(data_to_split)]==4))
range_of_matches[4,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==4))
}
#range_of_matches <- range_of_matches[, colSums(range_of_matches != 0) > 0]
gini = c()
for(j in colnames(range_of_matches)){
N1 = 0
N2 = 0
N1 = 1 - (range_of_matches[1,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2 - (range_of_matches[2,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2
N2 = 1 - (range_of_matches[3,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2 - (range_of_matches[4,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2
gini[j]= (((range_of_matches[1,j] + range_of_matches[2,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N1 ) +
(((range_of_matches[3,j] + range_of_matches[4,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N2 )
}
#end change
range_of_matches = rbind(range_of_matches , gini)
#minVal = min(gini[gini > 0])
minVal = min(gini)
index_to_split = c()
index_to_split[col_value] = which(range_of_matches['gini',]==minVal)[1]
tree_nodes_and_values <- c(tree_nodes_and_values , col_value , index_to_split[col_value])
data_to_split = subset(data_to_split[,],data_to_split[,col_value]<= index_to_split[col_value])
data_to_split <- data_to_split[,-col_value]
node_no = node_no + 1
#break condition
#better version of code
if(is.null(ncol(data_to_split)) ||
(ncol(data_to_split) <= 1) ||
(nrow(data_to_split) == 0) ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 2 ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 4)
{
return(tree_nodes_and_values)
}else{
return(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
}
}
#function to run test data based on the classification tree
test_data_check <- function(final_pred_data, final_list)
{
for(i in 1:length(names(final_list[seq(2, length(final_list), 2)]))){
final_pred_data[subset(final_pred_data[,],final_pred_data[,names(final_list[seq(2, length(final_list), 2)])[i]] > final_list[seq(3, length(final_list), 2)][[i]]), ncol(final_pred_data)] = 4
}
final_pred_data[which(final_pred_data[,ncol(final_pred_data)] == 0),ncol(final_pred_data)] = 2
count_tp <- 0
count_tn <- 0
count_fp <- 0
count_fn <- 0
confusion_mat <- matrix(0,2,2)
for(i in 1:nrow(final_pred_data)){
if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 2){
count_tp <- count_tp + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 4) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_tn <- count_tn + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_fp <- count_fp + 1
} else{
count_fn <- count_fn + 1
}
}
rownames(confusion_mat) <- c(2,4)
colnames(confusion_mat) <- c(2,4)
confusion_mat[1,1] <- count_tp
confusion_mat[2,2] <- count_tn
confusion_mat[1,2] <- count_fn
confusion_mat[2,1] <- count_fp
accuracy = (count_tp + count_tn)/(count_tp+count_tn+count_fp+count_fn)
predicted_mat <- list("predicted_mat"= final_pred_data , "confusion_mat" = confusion_mat, "accuracy"=accuracy)
return(predicted_mat)
}
breast.cancer.data = read.table("breast-cancer-wisconsin.data", header=FALSE,sep = ",")
colnames(breast.cancer.data) <- c("Sample code number", "Clump Thickness","Uniformity of Cell Size","Uniformity of Cell Shape","Marginal Adhesion","Single Epithelial Cell Size","Bare Nuclei","Bland Chromatin","Normal Nucleoli","Mitoses","Class")
breast.cancer.data$`Bare Nuclei`[breast.cancer.data$`Bare Nuclei` == "?"] = 1
totalData <- divideTrainTest(breast.cancer.data)
train_data = data.matrix(totalData$train_data)
train_data<-train_data[,-c(1)]
colnames(train_data)<-c(1:ncol(train_data))
rownames(train_data)<-c(1:nrow(train_data))
test_data = totalData$test_data
data_to_split <- train_data
class_node <- ncol(data_to_split)
#root node
node_no <- new.env()
node_no <- 0
#map to store column and tree splits
tree_map <- new.hashtable()
tree_nodes_and_values <- new.env()
max_data_val <- max(data_to_split)
#call classification tree algo which generate best features to split on and the corresponding values
final_list = as.list(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
total_nodes = c(names(final_list[seq(2, length(final_list), 2)]))
#corresponding values to split on
total_values = c(final_list[seq(3, length(final_list), 2)])
#features / nodes
total_nodes
#corresponding values of those nodes
total_values
#prediction code starts
final_pred_data <- data.matrix(test_data[,-1])
pred_class <- rep(0,nrow(final_pred_data))
final_pred_data <- cbind(final_pred_data,pred_class)
colnames(final_pred_data)<-c(1:ncol(final_pred_data))
rownames(final_pred_data)<-c(1:nrow(final_pred_data))
predicted_mat <- c()
#run test mat on classification tree
predicted_mat <- test_data_check(final_pred_data,final_list)
predicted_mat$'predicted_mat'
#confusion matrix
predicted_mat$'confusion_mat'
#accuracy of algo
predicted_mat$'accuracy'
#using the rpartfunction which draws the tree
library(rpart)
# grow tree
train_data <- as.data.frame(train_data)
fit <- rpart(train_data[,10] ~ train_data[,1] + train_data[,2] + train_data[,3] + train_data[,4] + train_data[,5] + train_data[,6] + train_data[,7] + train_data[,8] + train_data[,9],
method="class", data=train_data)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main="Classification Tree for breast cancer data")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
##Code for ROC :
library(pROC)
plot.new()
par(mfrow=c(1, 2))
rocobj = roc(predicted_mat$'predicted_mat'[,10],predicted_mat$'predicted_mat'[,11])
plot(rocobj,col=c("green"),main="ROC Curve")
## Code for recall
library(ROCR)
y <- predicted_mat$'predicted_mat'[,10] == 2
pred <- prediction(predicted_mat$'predicted_mat'[,11], y)
perf <- performance(pred, "prec", "rec")
plot (perf,col=c("red"),main="Precision Call")
new.hashtable <- function() {
e <- new.env()
list(set = function(key, value) assign(as.character(key), value, e),
get = function(key) get(as.character(key), e),
rm = function(key) rm(as.character(key), e))
}
#Division into test and train data
divideTrainTest <- function(original_data){
## 75% of the sample size
smp_size <- floor(0.75 * nrow(original_data))
## set the seed to make your partition reproductible
set.seed(123)
train_originial <- sample(seq_len(nrow(original_data)), size = smp_size)
train_data <- original_data[train_originial, ]
test_data <- original_data[-train_originial, ]
total_data <- list("train_data"= train_data , "test_data" = test_data)
return(total_data)
}
#finds best feature and value of that feature to split on
best_attribute_and_split <- function(data_to_split,tree_nodes_and_values,node_no,max_data_val)
{
unq_values<- c()
unq_class_values<- c()
unq_values <- sort(unique(as.vector(data_to_split)))
unq_class_values <- sort(unique(as.vector(data_to_split[,ncol(data_to_split)])))
matches_map <- new.hashtable()
for(i in unq_class_values)
{
count_of_val_with_class<- matrix(0,max_data_val,ncol(data_to_split))
colnames(count_of_val_with_class) <- as.integer(colnames(data_to_split))
for(k in colnames(data_to_split))
{
for(j in 1:max_data_val)
{
count_of_val_with_class[j,k] = length(subset(data_to_split[,k],data_to_split[,k]==j & data_to_split[,ncol(data_to_split)]==i))
}
}
matches_map$set(i, count_of_val_with_class)
}
count_of_val_with_class[is.na(count_of_val_with_class)] <- 0
prob_of_each_val<-  matrix(0,max_data_val,ncol(data_to_split))
colnames(prob_of_each_val) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
prob_of_each_val[i,j]<-(length(subset(data_to_split[,j],data_to_split[,j]==i)))/nrow(data_to_split)
}
}
prob_of_each_val[is.na(prob_of_each_val)] <- 0
prob_of_each_multiplied<- matrix(1,max_data_val,ncol(data_to_split))
colnames(prob_of_each_multiplied) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
for(k in unq_class_values){
prob_of_each_multiplied[i,j] <- prob_of_each_multiplied[i,j] * (matches_map$get(k)[i,j]/(length(subset(data_to_split[,j],data_to_split[,j]==i))))
}
}
}
prob_of_each_multiplied[is.na(prob_of_each_multiplied)] <- 0
gini_value_each_attr<- matrix(0,max_data_val,ncol(data_to_split))
colnames(gini_value_each_attr) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
gini_value_each_attr[i,j]<-  prob_of_each_val[i,j]*prob_of_each_multiplied[i,j]
}
}
find_gini_col = c()
for(j in colnames(data_to_split)){
sum = 0
for(i in 1:max_data_val){
sum = sum + gini_value_each_attr[i,j]
}
find_gini_col[j] = sum
}
prod = 1
for(u in unq_class_values){
prod = prod * prob_of_each_val[u,ncol(data_to_split)]
}
find_gini_col[ncol(data_to_split)] = prod
find_gini_gain = c()
for(v in colnames(data_to_split)){
find_gini_gain[v] = find_gini_col[ncol(data_to_split)] - find_gini_col[v]
}
##best attribute to split
col_value = c()
col_value = which.max(find_gini_gain)
split_col_values <- c()
split_col_values[node_no] = col_value
##Start for splitting the values of the best attribute/feature
unq_values <- sort(unique(as.vector(data_to_split[,col_value])))
avg_values <- c()
for(i in unq_values){
avg_values[i+1] <- ((i) + (i+1))/2
}
avg_values <- avg_values[!is.na(avg_values)]
avg_values <- avg_values[1:length(avg_values)-1]
range_of_matches<- matrix(0,length(unq_class_values)*2,length(avg_values))
colnames(range_of_matches) <- as.integer(floor(avg_values))
#change according to every data set
for(i in avg_values){
# i =2.5
range_of_matches[1,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<= i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[2,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[3,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<=i & data_to_split[,ncol(data_to_split)]==4))
range_of_matches[4,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==4))
}
#range_of_matches <- range_of_matches[, colSums(range_of_matches != 0) > 0]
gini = c()
for(j in colnames(range_of_matches)){
N1 = 0
N2 = 0
N1 = 1 - (range_of_matches[1,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2 - (range_of_matches[2,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2
N2 = 1 - (range_of_matches[3,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2 - (range_of_matches[4,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2
gini[j]= (((range_of_matches[1,j] + range_of_matches[2,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N1 ) +
(((range_of_matches[3,j] + range_of_matches[4,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N2 )
}
#end change
range_of_matches = rbind(range_of_matches , gini)
#minVal = min(gini[gini > 0])
minVal = min(gini)
index_to_split = c()
index_to_split[col_value] = which(range_of_matches['gini',]==minVal)[1]
tree_nodes_and_values <- c(tree_nodes_and_values , col_value , index_to_split[col_value])
data_to_split = subset(data_to_split[,],data_to_split[,col_value]<= index_to_split[col_value])
data_to_split <- data_to_split[,-col_value]
node_no = node_no + 1
#break condition
#better version of code
if(is.null(ncol(data_to_split)) ||
(ncol(data_to_split) <= 1) ||
(nrow(data_to_split) == 0) ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 2 ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 4)
{
return(tree_nodes_and_values)
}else{
return(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
}
}
#function to run test data based on the classification tree
test_data_check <- function(final_pred_data, final_list)
{
for(i in 1:length(names(final_list[seq(2, length(final_list), 2)]))){
final_pred_data[subset(final_pred_data[,],final_pred_data[,names(final_list[seq(2, length(final_list), 2)])[i]] > final_list[seq(3, length(final_list), 2)][[i]]), ncol(final_pred_data)] = 4
}
final_pred_data[which(final_pred_data[,ncol(final_pred_data)] == 0),ncol(final_pred_data)] = 2
count_tp <- 0
count_tn <- 0
count_fp <- 0
count_fn <- 0
confusion_mat <- matrix(0,2,2)
for(i in 1:nrow(final_pred_data)){
if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 2){
count_tp <- count_tp + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 4) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_tn <- count_tn + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_fp <- count_fp + 1
} else{
count_fn <- count_fn + 1
}
}
rownames(confusion_mat) <- c(2,4)
colnames(confusion_mat) <- c(2,4)
confusion_mat[1,1] <- count_tp
confusion_mat[2,2] <- count_tn
confusion_mat[1,2] <- count_fn
confusion_mat[2,1] <- count_fp
accuracy = (count_tp + count_tn)/(count_tp+count_tn+count_fp+count_fn)
predicted_mat <- list("predicted_mat"= final_pred_data , "confusion_mat" = confusion_mat, "accuracy"=accuracy)
return(predicted_mat)
}
breast.cancer.data = read.table("breast-cancer-wisconsin.data", header=FALSE,sep = ",")
colnames(breast.cancer.data) <- c("Sample code number", "Clump Thickness","Uniformity of Cell Size","Uniformity of Cell Shape","Marginal Adhesion","Single Epithelial Cell Size","Bare Nuclei","Bland Chromatin","Normal Nucleoli","Mitoses","Class")
breast.cancer.data$`Bare Nuclei`[breast.cancer.data$`Bare Nuclei` == "?"] = 1
totalData <- divideTrainTest(breast.cancer.data)
train_data = data.matrix(totalData$train_data)
train_data<-train_data[,-c(1)]
colnames(train_data)<-c(1:ncol(train_data))
rownames(train_data)<-c(1:nrow(train_data))
test_data = totalData$test_data
data_to_split <- train_data
class_node <- ncol(data_to_split)
#root node
node_no <- new.env()
node_no <- 0
#map to store column and tree splits
tree_map <- new.hashtable()
tree_nodes_and_values <- new.env()
max_data_val <- max(data_to_split)
#call classification tree algo which generate best features to split on and the corresponding values
final_list = as.list(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
total_nodes = c(names(final_list[seq(2, length(final_list), 2)]))
#corresponding values to split on
total_values = c(final_list[seq(3, length(final_list), 2)])
#features / nodes
total_nodes
#corresponding values of those nodes
total_values
#prediction code starts
final_pred_data <- data.matrix(test_data[,-1])
pred_class <- rep(0,nrow(final_pred_data))
final_pred_data <- cbind(final_pred_data,pred_class)
colnames(final_pred_data)<-c(1:ncol(final_pred_data))
rownames(final_pred_data)<-c(1:nrow(final_pred_data))
predicted_mat <- c()
#run test mat on classification tree
predicted_mat <- test_data_check(final_pred_data,final_list)
predicted_mat$'predicted_mat'
#confusion matrix
predicted_mat$'confusion_mat'
#accuracy of algo
predicted_mat$'accuracy'
#using the rpartfunction which draws the tree
library(rpart)
# grow tree
train_data <- as.data.frame(train_data)
fit <- rpart(train_data[,10] ~ train_data[,1] + train_data[,2] + train_data[,3] + train_data[,4] + train_data[,5] + train_data[,6] + train_data[,7] + train_data[,8] + train_data[,9],
method="class", data=train_data)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main="Classification Tree for breast cancer data")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
##Code for ROC :
library(pROC)
plot.new()
par(mfrow=c(1, 2))
rocobj = roc(predicted_mat$'predicted_mat'[,10],predicted_mat$'predicted_mat'[,11])
plot(rocobj,col=c("green"),main="ROC Curve")
## Code for recall
library(ROCR)
y <- predicted_mat$'predicted_mat'[,10] == 2
pred <- prediction(predicted_mat$'predicted_mat'[,11], y)
perf <- performance(pred, "prec", "rec")
plot (perf,col=c("red"),main="Precision Call")
new.hashtable <- function() {
e <- new.env()
list(set = function(key, value) assign(as.character(key), value, e),
get = function(key) get(as.character(key), e),
rm = function(key) rm(as.character(key), e))
}
#Division into test and train data
divideTrainTest <- function(original_data){
## 75% of the sample size
smp_size <- floor(0.75 * nrow(original_data))
## set the seed to make your partition reproductible
set.seed(123)
train_originial <- sample(seq_len(nrow(original_data)), size = smp_size)
train_data <- original_data[train_originial, ]
test_data <- original_data[-train_originial, ]
total_data <- list("train_data"= train_data , "test_data" = test_data)
return(total_data)
}
best_attribute_and_split <- function(data_to_split,tree_nodes_and_values,node_no,max_data_val)
{
unq_values<- c()
unq_class_values<- c()
unq_values <- sort(unique(as.vector(data_to_split)))
unq_class_values <- sort(unique(as.vector(data_to_split[,ncol(data_to_split)])))
matches_map <- new.hashtable()
for(i in unq_class_values)
{
count_of_val_with_class<- matrix(0,max_data_val,ncol(data_to_split))
colnames(count_of_val_with_class) <- as.integer(colnames(data_to_split))
for(k in colnames(data_to_split))
{
for(j in 1:max_data_val)
{
count_of_val_with_class[j,k] = length(subset(data_to_split[,k],data_to_split[,k]==j & data_to_split[,ncol(data_to_split)]==i))
}
}
matches_map$set(i, count_of_val_with_class)
}
count_of_val_with_class[is.na(count_of_val_with_class)] <- 0
prob_of_each_val<-  matrix(0,max_data_val,ncol(data_to_split))
colnames(prob_of_each_val) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
prob_of_each_val[i,j]<-(length(subset(data_to_split[,j],data_to_split[,j]==i)))/nrow(data_to_split)
}
}
prob_of_each_val[is.na(prob_of_each_val)] <- 0
prob_of_each_multiplied<- matrix(1,max_data_val,ncol(data_to_split))
colnames(prob_of_each_multiplied) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
for(k in unq_class_values){
prob_of_each_multiplied[i,j] <- prob_of_each_multiplied[i,j] * (matches_map$get(k)[i,j]/(length(subset(data_to_split[,j],data_to_split[,j]==i))))
}
}
}
prob_of_each_multiplied[is.na(prob_of_each_multiplied)] <- 0
gini_value_each_attr<- matrix(0,max_data_val,ncol(data_to_split))
colnames(gini_value_each_attr) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
gini_value_each_attr[i,j]<-  prob_of_each_val[i,j]*prob_of_each_multiplied[i,j]
}
}
find_gini_col = c()
for(j in colnames(data_to_split)){
sum = 0
for(i in 1:max_data_val){
sum = sum + gini_value_each_attr[i,j]
}
find_gini_col[j] = sum
}
prod = 1
for(u in unq_class_values){
prod = prod * prob_of_each_val[u,ncol(data_to_split)]
}
find_gini_col[ncol(data_to_split)] = prod
find_gini_gain = c()
for(v in colnames(data_to_split)){
find_gini_gain[v] = find_gini_col[ncol(data_to_split)] - find_gini_col[v]
}
##best attribute to split
col_value = c()
col_value = which.max(find_gini_gain)
split_col_values <- c()
split_col_values[node_no] = col_value
##Start for splitting the values of the best attribute/feature
unq_values <- sort(unique(as.vector(data_to_split[,col_value])))
avg_values <- c()
for(i in unq_values){
avg_values[i+1] <- ((i) + (i+1))/2
}
avg_values <- avg_values[!is.na(avg_values)]
avg_values <- avg_values[1:length(avg_values)-1]
range_of_matches<- matrix(0,length(unq_class_values)*2,length(avg_values))
colnames(range_of_matches) <- as.integer(floor(avg_values))
#change according to every data set
for(i in avg_values){
# i =2.5
range_of_matches[1,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<= i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[2,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[3,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<=i & data_to_split[,ncol(data_to_split)]==4))
range_of_matches[4,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==4))
}
setwd("~/Desktop/1/Tanikanti, Aditya (atanikan)/20160226_1156PM (7947170)/Assignment 2 Solutions")
R.version
R.version
install.packages("caret")
library(caret)
install.packages("mlbench")
library(mlbench)
data= read.table("alldataset.csv",sep=",", header = TRUE)
setwd("~/Documents/RESEARCH/2016/EM-heapEM/EM-python-SON/EMMM")
data= read.table("alldataset.csv",sep=",", header = TRUE)
label <- data[,2]
data <- data[,3:34]
# calculate correlation matrix
correlationMatrix <- cor(data)
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
highlyCorrelated
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(data, data=data, method="lvq", preProcess="scale", trControl=control)
data(PimaIndiansDiabetes)
PimaIndiansDiabetes
model <- train(label, data=data, method="lvq", preProcess="scale", trControl=control)
data= read.table("alldataset.csv",sep=",", header = TRUE)
data <- data[,2:34]
model <- train(data[,1]~., data=data, method="lvq", preProcess="scale", trControl=control)
dim(data)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
results <- rfe(data[,2:33], data[,1], sizes=c(2:33), rfeControl=control)
install.packages("randomForest")
library(randomForest)
results <- rfe(data[,2:33], data[,1], sizes=c(2:33), rfeControl=control)
