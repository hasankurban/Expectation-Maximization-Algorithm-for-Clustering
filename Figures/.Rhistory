matches_map$set(i, count_of_val_with_class)
}
count_of_val_with_class[is.na(count_of_val_with_class)] <- 0
prob_of_each_val<-  matrix(0,max_data_val,ncol(data_to_split))
colnames(prob_of_each_val) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
prob_of_each_val[i,j]<-(length(subset(data_to_split[,j],data_to_split[,j]==i)))/nrow(data_to_split)
}
}
prob_of_each_val[is.na(prob_of_each_val)] <- 0
prob_of_each_multiplied<- matrix(1,max_data_val,ncol(data_to_split))
colnames(prob_of_each_multiplied) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
for(k in unq_class_values){
prob_of_each_multiplied[i,j] <- prob_of_each_multiplied[i,j] * (matches_map$get(k)[i,j]/(length(subset(data_to_split[,j],data_to_split[,j]==i))))
}
}
}
prob_of_each_multiplied[is.na(prob_of_each_multiplied)] <- 0
gini_value_each_attr<- matrix(0,max_data_val,ncol(data_to_split))
colnames(gini_value_each_attr) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
gini_value_each_attr[i,j]<-  prob_of_each_val[i,j]*prob_of_each_multiplied[i,j]
}
}
find_gini_col = c()
for(j in colnames(data_to_split)){
sum = 0
for(i in 1:max_data_val){
sum = sum + gini_value_each_attr[i,j]
}
find_gini_col[j] = sum
}
prod = 1
for(u in unq_class_values){
prod = prod * prob_of_each_val[u,ncol(data_to_split)]
}
find_gini_col[ncol(data_to_split)] = prod
find_gini_gain = c()
for(v in colnames(data_to_split)){
find_gini_gain[v] = find_gini_col[ncol(data_to_split)] - find_gini_col[v]
}
##best attribute to split
col_value = c()
col_value = which.max(find_gini_gain)
split_col_values <- c()
split_col_values[node_no] = col_value
##Start for splitting the values of the best attribute/feature
unq_values <- sort(unique(as.vector(data_to_split[,col_value])))
avg_values <- c()
for(i in unq_values){
avg_values[i+1] <- ((i) + (i+1))/2
}
avg_values <- avg_values[!is.na(avg_values)]
avg_values <- avg_values[1:length(avg_values)-1]
range_of_matches<- matrix(0,length(unq_class_values)*2,length(avg_values))
colnames(range_of_matches) <- as.integer(floor(avg_values))
#change according to every data set
for(i in avg_values){
# i =2.5
range_of_matches[1,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<= i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[2,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[3,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<=i & data_to_split[,ncol(data_to_split)]==4))
range_of_matches[4,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==4))
}
#range_of_matches <- range_of_matches[, colSums(range_of_matches != 0) > 0]
gini = c()
for(j in colnames(range_of_matches)){
N1 = 0
N2 = 0
N1 = 1 - (range_of_matches[1,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2 - (range_of_matches[2,j]/(range_of_matches[1,j]+range_of_matches[2,j]))^2
N2 = 1 - (range_of_matches[3,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2 - (range_of_matches[4,j]/(range_of_matches[3,j]+range_of_matches[4,j]))^2
gini[j]= (((range_of_matches[1,j] + range_of_matches[2,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N1 ) +
(((range_of_matches[3,j] + range_of_matches[4,j])/(range_of_matches[1,j] + range_of_matches[2,j] +range_of_matches[3,j] + range_of_matches[4,j])) * N2 )
}
#end change
range_of_matches = rbind(range_of_matches , gini)
#minVal = min(gini[gini > 0])
minVal = min(gini)
index_to_split = c()
index_to_split[col_value] = which(range_of_matches['gini',]==minVal)[1]
tree_nodes_and_values <- c(tree_nodes_and_values , col_value , index_to_split[col_value])
data_to_split = subset(data_to_split[,],data_to_split[,col_value]<= index_to_split[col_value])
data_to_split <- data_to_split[,-col_value]
node_no = node_no + 1
#break condition
#better version of code
if(is.null(ncol(data_to_split)) ||
(ncol(data_to_split) <= 1) ||
(nrow(data_to_split) == 0) ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 2 ||
all(data_to_split[,tail(colnames(data_to_split),1)]) == 4)
{
return(tree_nodes_and_values)
}else{
return(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
}
}
#function to run test data based on the classification tree
test_data_check <- function(final_pred_data, final_list)
{
for(i in 1:length(names(final_list[seq(2, length(final_list), 2)]))){
final_pred_data[subset(final_pred_data[,],final_pred_data[,names(final_list[seq(2, length(final_list), 2)])[i]] > final_list[seq(3, length(final_list), 2)][[i]]), ncol(final_pred_data)] = 4
}
final_pred_data[which(final_pred_data[,ncol(final_pred_data)] == 0),ncol(final_pred_data)] = 2
count_tp <- 0
count_tn <- 0
count_fp <- 0
count_fn <- 0
confusion_mat <- matrix(0,2,2)
for(i in 1:nrow(final_pred_data)){
if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 2){
count_tp <- count_tp + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 4) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_tn <- count_tn + 1
} else if((final_pred_data[i,ncol(final_pred_data)] == 2) & (final_pred_data[i,ncol(final_pred_data)-1]) == 4){
count_fp <- count_fp + 1
} else{
count_fn <- count_fn + 1
}
}
rownames(confusion_mat) <- c(2,4)
colnames(confusion_mat) <- c(2,4)
confusion_mat[1,1] <- count_tp
confusion_mat[2,2] <- count_tn
confusion_mat[1,2] <- count_fn
confusion_mat[2,1] <- count_fp
accuracy = (count_tp + count_tn)/(count_tp+count_tn+count_fp+count_fn)
predicted_mat <- list("predicted_mat"= final_pred_data , "confusion_mat" = confusion_mat, "accuracy"=accuracy)
return(predicted_mat)
}
breast.cancer.data = read.table("breast-cancer-wisconsin.data", header=FALSE,sep = ",")
colnames(breast.cancer.data) <- c("Sample code number", "Clump Thickness","Uniformity of Cell Size","Uniformity of Cell Shape","Marginal Adhesion","Single Epithelial Cell Size","Bare Nuclei","Bland Chromatin","Normal Nucleoli","Mitoses","Class")
breast.cancer.data$`Bare Nuclei`[breast.cancer.data$`Bare Nuclei` == "?"] = 1
totalData <- divideTrainTest(breast.cancer.data)
train_data = data.matrix(totalData$train_data)
train_data<-train_data[,-c(1)]
colnames(train_data)<-c(1:ncol(train_data))
rownames(train_data)<-c(1:nrow(train_data))
test_data = totalData$test_data
data_to_split <- train_data
class_node <- ncol(data_to_split)
#root node
node_no <- new.env()
node_no <- 0
#map to store column and tree splits
tree_map <- new.hashtable()
tree_nodes_and_values <- new.env()
max_data_val <- max(data_to_split)
#call classification tree algo which generate best features to split on and the corresponding values
final_list = as.list(best_attribute_and_split(data_to_split,tree_nodes_and_values, node_no, max_data_val))
total_nodes = c(names(final_list[seq(2, length(final_list), 2)]))
#corresponding values to split on
total_values = c(final_list[seq(3, length(final_list), 2)])
#features / nodes
total_nodes
#corresponding values of those nodes
total_values
#prediction code starts
final_pred_data <- data.matrix(test_data[,-1])
pred_class <- rep(0,nrow(final_pred_data))
final_pred_data <- cbind(final_pred_data,pred_class)
colnames(final_pred_data)<-c(1:ncol(final_pred_data))
rownames(final_pred_data)<-c(1:nrow(final_pred_data))
predicted_mat <- c()
#run test mat on classification tree
predicted_mat <- test_data_check(final_pred_data,final_list)
predicted_mat$'predicted_mat'
#confusion matrix
predicted_mat$'confusion_mat'
#accuracy of algo
predicted_mat$'accuracy'
#using the rpartfunction which draws the tree
library(rpart)
# grow tree
train_data <- as.data.frame(train_data)
fit <- rpart(train_data[,10] ~ train_data[,1] + train_data[,2] + train_data[,3] + train_data[,4] + train_data[,5] + train_data[,6] + train_data[,7] + train_data[,8] + train_data[,9],
method="class", data=train_data)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main="Classification Tree for breast cancer data")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
##Code for ROC :
library(pROC)
plot.new()
par(mfrow=c(1, 2))
rocobj = roc(predicted_mat$'predicted_mat'[,10],predicted_mat$'predicted_mat'[,11])
plot(rocobj,col=c("green"),main="ROC Curve")
## Code for recall
library(ROCR)
y <- predicted_mat$'predicted_mat'[,10] == 2
pred <- prediction(predicted_mat$'predicted_mat'[,11], y)
perf <- performance(pred, "prec", "rec")
plot (perf,col=c("red"),main="Precision Call")
new.hashtable <- function() {
e <- new.env()
list(set = function(key, value) assign(as.character(key), value, e),
get = function(key) get(as.character(key), e),
rm = function(key) rm(as.character(key), e))
}
#Division into test and train data
divideTrainTest <- function(original_data){
## 75% of the sample size
smp_size <- floor(0.75 * nrow(original_data))
## set the seed to make your partition reproductible
set.seed(123)
train_originial <- sample(seq_len(nrow(original_data)), size = smp_size)
train_data <- original_data[train_originial, ]
test_data <- original_data[-train_originial, ]
total_data <- list("train_data"= train_data , "test_data" = test_data)
return(total_data)
}
best_attribute_and_split <- function(data_to_split,tree_nodes_and_values,node_no,max_data_val)
{
unq_values<- c()
unq_class_values<- c()
unq_values <- sort(unique(as.vector(data_to_split)))
unq_class_values <- sort(unique(as.vector(data_to_split[,ncol(data_to_split)])))
matches_map <- new.hashtable()
for(i in unq_class_values)
{
count_of_val_with_class<- matrix(0,max_data_val,ncol(data_to_split))
colnames(count_of_val_with_class) <- as.integer(colnames(data_to_split))
for(k in colnames(data_to_split))
{
for(j in 1:max_data_val)
{
count_of_val_with_class[j,k] = length(subset(data_to_split[,k],data_to_split[,k]==j & data_to_split[,ncol(data_to_split)]==i))
}
}
matches_map$set(i, count_of_val_with_class)
}
count_of_val_with_class[is.na(count_of_val_with_class)] <- 0
prob_of_each_val<-  matrix(0,max_data_val,ncol(data_to_split))
colnames(prob_of_each_val) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
prob_of_each_val[i,j]<-(length(subset(data_to_split[,j],data_to_split[,j]==i)))/nrow(data_to_split)
}
}
prob_of_each_val[is.na(prob_of_each_val)] <- 0
prob_of_each_multiplied<- matrix(1,max_data_val,ncol(data_to_split))
colnames(prob_of_each_multiplied) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
for(k in unq_class_values){
prob_of_each_multiplied[i,j] <- prob_of_each_multiplied[i,j] * (matches_map$get(k)[i,j]/(length(subset(data_to_split[,j],data_to_split[,j]==i))))
}
}
}
prob_of_each_multiplied[is.na(prob_of_each_multiplied)] <- 0
gini_value_each_attr<- matrix(0,max_data_val,ncol(data_to_split))
colnames(gini_value_each_attr) <- as.integer(colnames(data_to_split))
for(j in colnames(data_to_split)){
for(i in 1:max_data_val){
gini_value_each_attr[i,j]<-  prob_of_each_val[i,j]*prob_of_each_multiplied[i,j]
}
}
find_gini_col = c()
for(j in colnames(data_to_split)){
sum = 0
for(i in 1:max_data_val){
sum = sum + gini_value_each_attr[i,j]
}
find_gini_col[j] = sum
}
prod = 1
for(u in unq_class_values){
prod = prod * prob_of_each_val[u,ncol(data_to_split)]
}
find_gini_col[ncol(data_to_split)] = prod
find_gini_gain = c()
for(v in colnames(data_to_split)){
find_gini_gain[v] = find_gini_col[ncol(data_to_split)] - find_gini_col[v]
}
##best attribute to split
col_value = c()
col_value = which.max(find_gini_gain)
split_col_values <- c()
split_col_values[node_no] = col_value
##Start for splitting the values of the best attribute/feature
unq_values <- sort(unique(as.vector(data_to_split[,col_value])))
avg_values <- c()
for(i in unq_values){
avg_values[i+1] <- ((i) + (i+1))/2
}
avg_values <- avg_values[!is.na(avg_values)]
avg_values <- avg_values[1:length(avg_values)-1]
range_of_matches<- matrix(0,length(unq_class_values)*2,length(avg_values))
colnames(range_of_matches) <- as.integer(floor(avg_values))
#change according to every data set
for(i in avg_values){
# i =2.5
range_of_matches[1,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<= i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[2,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==2))
range_of_matches[3,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]<=i & data_to_split[,ncol(data_to_split)]==4))
range_of_matches[4,as.character(floor(i))] = length(subset(data_to_split[,col_value],data_to_split[,col_value]>i & data_to_split[,ncol(data_to_split)]==4))
}
setwd("~/Desktop/1/Tanikanti, Aditya (atanikan)/20160226_1156PM (7947170)/Assignment 2 Solutions")
R.version
setwd("~/Desktop/kmeans-SIAM/kmeans")
data <- read.table("galaxySurveyDataSet.txt")
dim(data)
data <- read.table("galaxySurveyDataSet.txt",sep",")
data <- read.table("galaxySurveyDataSet.txt",sep=",")
dim(data)
head(data)
data[0]
data[0:]
data[0,]
data[1,]
head(data)
data <- read.table("galaxySurveyDataSet.txt",sep=",", header=TRUE)
head(data)
data[1,]
data[1800,]
View(data)
dim(data)
1800 * 500
data[180000]
data[180000,]
data[178200,]
data(iris)
pairs(iris[1:4],main="Iris Data", pch=19, col=as.numeric(iris$Species)+1)
iris.pca <- prcomp(log(iris[,1:4]), scale=TRUE, center=TRUE)
summary(iris.pca)
screeplot(iris.pca, type="lines", col="3")
iris.pca$sdev^2
iris
summary(data)
summary(data)
cor(data)
dim(data)
class(data)
?pairs
summary(data)
suymmary(iris)
summary(iris)
iris
summary(iris)
iris
summary(iris)
iris
summary(iris)
iris.pca <- prcomp(log(iris[,1:4]), scale=TRUE, center=TRUE)
summary(iris.pca)
iris.pca$sdev^2
1.7124583^2
summary(iris.pca)
iris.pca
data(iris)
pairs(iris[1:4],main="Iris Data", pch=19, col=as.numeric(iris$Species)+1)
iris.pca <- prcomp(log(iris[,1:4]), scale=TRUE, center=TRUE)
iris.pca
summary(iris.pca)
irc.pca$rotation
iris.pca$rotation
iris.pca
0.5210659^2+(-0.2693474)^2 +0.5804131^2 + 0.5648565^2
data1 = c(2,2)
data1 = c(-2,2)
data2 = (4,0)
data2 = c(4,0)
data2 = cbind(data1,data())
data2 = cbind(data1,data2_
data2 = cbind(data1,data2)
data2
data2 = c(4,0)
data2 = c(4,0)
data3 = as.data.frame(cbind(data1,data2))
data2
data1
data3
data3 = as.data.frame(rbind(data1,data2))
data3
cov(data)
cov(data3)
log(50000,2)
500000*16
getwd()
setwd("~/Desktop/newKmeans/R-Data/dataUsed/realData")
data <- read.table("spambase.txt",sep=",")
data[,58] <- as.factor(data[,58])
temp <- as.data.frame(scale(data[1:57]))
newData<- cbind(temp, data[,58])
write.csv(newData, file = "spambaseNorm.txt",row.names=FALSE)
sumamry(spambase)
summary(spambase)
summary(newData)
head(newData)
data <- read.table("spambase.txt",sep=",")
data[,58] <- as.factor(data[,58])
head(newData)
head(data)
summary(data())
summary(data
)
summary(newData)
2788 /4601
1- 0.6059552
data= read.table("ring.txt",sep=",", header = TRUE)
data[,21] <- as.factor(data[,21])
temp <- as.data.frame(scale(data[1:20]))
newData<- cbind(temp, data[,20])
summary(newData)
summary(data())
summary(data
)
dim(ring)
dim(ringnorm)
dim(dta)
dim(data)
3736/7400
round(3736/7400)
round(3736/7400,2)
round(3736/7400,3)
dim(data)
summary(data)
3736/7400
setwd("~/Downloads/ringnorm")
hasan = read.table("Dataset.data",sep=",")
head(hasan)
hasan = read.table("Dataset.data",sep=",",header=TRUE)
head(has)
head(hasan)
dim(hasan)
hasan = read.table("Dataset.data",sep=" ",header=FALSE)
hasan = read.table("Dataset.data",header=FALSE)
dim(hasan)
head(hasan)
12332/19020
1-12332/19020
log2(1) + log2(2) +log2(3) + log2(4)
log2(24)
17.09*427 + 17.84 * 420 + 251 *18.81
19511.54/ (427+420+251)
427+420+251)
427+420+251
19.55 -17.77
1.78 * 1098
22-17.77
4.23 * 1098
427 * 17.09 + 410 *17.84 + 251 * 18.81
19333.14/ (427+410+251)
457+ 35 + 60
251+427
678 * 19.26
13058.28  + 2598.54
427 * 19.26
2598 /3.64
7340/3.64
427 * 17.09 + 251 * 18.81
12018.74 /678
427 * 17.09 + 251 * 18.81 + 17.27 *134
4332.92 / (678+134)
14332.92 / (678+134)
427 * 17.09 + 251 * 18.81 + 19.27 *134
14600.92 / (678+134)
(678+134)
427 * 17.09 + 251 * 18.81 + 17.27 *134 + 100 *18.98 + 99 * 18.95
18106.97/1011
427 * 17.09 + 251 * 18.81 + 19.27 *134 + 100 *18.98 + 99 * 18.95
18374.97 /1011
18374.97 /(427+251+134+100+199)
18374.97 /(427+251+134+100+99)
(427+251+134+100+99)
457 + 58.03 + 59.41
379 / 6
379.19/6
574 + 63 + 35
584 * +427
584  +427
19.30 * 5.84
19.30 * 584
11271.2 / 18.70
11271.2 / 18.30
31 *20
19.50 * 584
11388 / 18.70
11388 / 18.40
618 + 427
19.27 * 134 + 19.98 * 100 + 18.95 * 99 + 18.81 * 251
134 + 200 + 250
19.27 * 134 + 19.98 * 100 + 18.95 * 99 + 18.81 * 251 / 584
19.27 * 134 + 18.98 * 100 + 18.95 * 99 + 18.81 * 251 / 584
(19.27 * 134 + 18.98 * 100 + 18.95 * 99 + 18.81 * 251 )/ 584
584 * ( 0.6)
584 *18.97
1078.48 /584
11078.48 /584
1011 *18.17 + 18.98 * 30
18939.27 /1041
1041 * 18.20 + 20 * 19
19326.2 /1061
3000/21
0.6 * 142
5000 * 3.55
17750 /5
400 *5
530 +460 +385
137500/1500
590 +570 +435
1595 /1600
91.67 * 5 +87 + 200*2 + 72.73
91.67 /2 + 8.7 + 20 +7.273
8200/90
